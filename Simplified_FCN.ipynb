{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple CNN model architecture for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "simple_cnn = Sequential()\n",
    "simple_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_1'))\n",
    "simple_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_2'))\n",
    "simple_cnn.add(MaxPooling2D(pool_size=(2, 2), name='pool1'))\n",
    "\n",
    "simple_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_1'))\n",
    "simple_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_2'))\n",
    "simple_cnn.add(MaxPooling2D(pool_size=(5, 2), name='pool2'))\n",
    "\n",
    "simple_cnn.add(Flatten(name='flatten'))\n",
    "simple_cnn.add(Dense(32, activation='relu', name='fc1'))\n",
    "simple_cnn.add(Dropout(0.5))\n",
    "simple_cnn.add(Dense(1, activation='sigmoid', name='prediction'))\n",
    "\n",
    "from keras import optimizers\n",
    "adam = optimizers.Adam(lr=1e-7, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "simple_cnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1_1 (Conv2D)             (None, 70, 116, 16)       160       \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 70, 116, 16)       2320      \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 35, 58, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 35, 58, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 35, 58, 32)        9248      \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 7, 29, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6496)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 32)                207904    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 224,305\n",
      "Trainable params: 224,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(simple_cnn, to_file='simple_cnn.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read small image files for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=180,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "        rotation_range=180, \n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/xenialxerus/cnn-medical-image-segmentation/train/',  \n",
    "        target_size=(70, 116), \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        color_mode='grayscale') \n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        '/home/xenialxerus/cnn-medical-image-segmentation/test/',\n",
    "        target_size=(70, 116),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification training for small images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.load_weights('simple_cnn_weights/simple_cnn_0_7903.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "# time-consuming training process, expect to run for a long time\n",
    "simple_cnn.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch= 1393544//256,\n",
    "        epochs=5,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=235343//256)\n",
    "simple_cnn.save_weights('simple_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "masked_imgs = [img for img in glob.glob(\"data_simple_cnn/test/mask/*\")]\n",
    "no_masked_imgs = [img for img in glob.glob(\"data_simple_cnn/test/no_mask/*\")]\n",
    "\n",
    "n_masked = len(masked_imgs)\n",
    "n_no_masked = len(no_masked_imgs)\n",
    "\n",
    "count_masked = 0\n",
    "for img in masked_imgs:\n",
    "    img = io.imread(img)\n",
    "    img = np.expand_dims(img, 0)\n",
    "    img = np.expand_dims(img, -1)\n",
    "    pred = simple_cnn.predict(img)\n",
    "    if pred[0][0] < 0.8:\n",
    "        count_masked += 1\n",
    "print(\"masked accuracy: \", float(count_masked) / n_masked)\n",
    "\n",
    "count_no_masked = 0\n",
    "for img in no_masked_imgs:\n",
    "    img = io.imread(img)\n",
    "    img = np.expand_dims(img, 0)\n",
    "    img = np.expand_dims(img, -1)\n",
    "    pred = simple_cnn.predict(img)\n",
    "    if pred[0][0] < 0.8:\n",
    "        count_no_masked += 1\n",
    "print(\"no_masked accuracy: \", float(count_no_masked) / n_no_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image segmention loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.objectives import *\n",
    "import tensorflow as tf\n",
    "\n",
    "def binary_crossentropy_with_logits(ground_truth, predictions):\n",
    "    return K.mean(K.binary_crossentropy(ground_truth,\n",
    "                                        predictions,\n",
    "                                        from_logits=True),\n",
    "                  axis=-1)\n",
    "\n",
    "\n",
    "def softmax_sparse_crossentropy_ignoring_last_label(y_true, y_pred):\n",
    "    y_pred = K.reshape(y_pred, (-1, K.int_shape(y_pred)[-1]))\n",
    "    log_softmax = tf.nn.log_softmax(y_pred)\n",
    "\n",
    "    y_true = K.one_hot(tf.to_int32(K.flatten(y_true)), K.int_shape(y_pred)[-1]+1)\n",
    "    unpacked = tf.unstack(y_true, axis=-1)\n",
    "    y_true = tf.stack(unpacked[:-1], axis=-1)\n",
    "\n",
    "    cross_entropy = -K.sum(y_true * log_softmax, axis=1)\n",
    "    cross_entropy_mean = K.mean(cross_entropy)\n",
    "\n",
    "    return cross_entropy_mean\n",
    "\n",
    "def sparse_accuracy_ignoring_last_label(y_true, y_pred):\n",
    "    nb_classes = K.int_shape(y_pred)[-1]\n",
    "    y_pred = K.reshape(y_pred, (-1, nb_classes))\n",
    "\n",
    "    y_true = K.one_hot(tf.to_int32(K.flatten(y_true)),\n",
    "                       nb_classes + 1)\n",
    "    unpacked = tf.unstack(y_true, axis=-1)\n",
    "    legal_labels = ~tf.cast(unpacked[-1], tf.bool)\n",
    "    y_true = tf.stack(unpacked[:-1], axis=-1)\n",
    "\n",
    "    return K.sum(tf.to_float(legal_labels & K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))) / K.sum(tf.to_float(legal_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN model architecture for small image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, Add, Dropout, UpSampling2D\n",
    "\n",
    "'''\n",
    "simple_cnn = Sequential()\n",
    "simple_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_1'))\n",
    "simple_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_2'))\n",
    "simple_cnn.add(MaxPooling2D(pool_size=(2, 2), name='pool1'))\n",
    "\n",
    "simple_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_1'))\n",
    "simple_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_2'))\n",
    "simple_cnn.add(MaxPooling2D(pool_size=(5, 2), name='pool2'))\n",
    "\n",
    "simple_cnn.add(Flatten(name='flatten'))\n",
    "simple_cnn.add(Dense(32, activation='relu', name='fc1'))\n",
    "simple_cnn.add(Dropout(0.5))\n",
    "simple_cnn.add(Dense(1, activation='sigmoid', name='prediction'))\n",
    "'''\n",
    "def fcn_cnn():\n",
    "    fc_cnn = Sequential() \n",
    "    fc_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_1'))\n",
    "    fc_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_2'))\n",
    "    fc_cnn.add(MaxPooling2D(pool_size=(2, 2), name='pool1')) # (35, 58, 1)\n",
    "\n",
    "    fc_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_1'))\n",
    "    fc_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_2'))\n",
    "    fc_cnn.add(MaxPooling2D(pool_size=(5, 2), name='pool2')) # (7, 29, 1)\n",
    "\n",
    "    # continue to use convoluational layers instead of fully connected layers\n",
    "    fc_cnn.add(Conv2D(128, (7, 29), padding='same', activation='relu', name='fc3'))\n",
    "    fc_cnn.add(Dropout(0.5))\n",
    "    fc_cnn.add(Conv2D(128, (1, 1), padding='same', activation='relu', name='fc4'))\n",
    "    fc_cnn.add(Dropout(0.5))\n",
    "\n",
    "    fc_cnn.add(Conv2D(2, (1, 1), padding='same', name='logit_fc4')) # [7, 29, 2]\n",
    "    # deconv_logit_fc4 by factor [5, 2] to [35, 58, 2]\n",
    "    # fc_cnn.add(Conv2DTranspose(2, kernel_size=(2*5-5%2, 2*2-2%2), strides=(5, 2), padding='same', name='deconv_logit_fc4'))\n",
    "    # use UpSampling2D instead of Conv2DTranspose\n",
    "    fc_cnn.add(UpSampling2D(size=(10, 4), name='score_fr'))\n",
    "    \n",
    "    # conv logit from pool1 to [35, 58, 2]\n",
    "    #logit_pool1 = Conv2D(2, (1, 1), padding='same', name='logit_pool1')(fc_cnn.layers[2].output)\n",
    "    # add deconv_logits_fc4 and logit_pool1\n",
    "    #logit_pool1_deconv_logit_fc4 = Add()([logit_pool1, fc_cnn.layers[-1].output])\n",
    "    # deconv above sum by [2, 2] to [70, 116, 2] --> pixel-wise classification: segmentation logit\n",
    "    #final_deconv = Conv2DTranspose(2, kernel_size=(2*2-2%2, 2*2-2%2), strides=(2, 2), \n",
    "     #                              padding='same', name='final_deconv')(logit_pool1_deconv_logit_fc4)\n",
    "    \n",
    "    #return Model(fc_cnn.input, final_deconv)\n",
    "    return fc_cnn\n",
    "    \n",
    "fcn = fcn_cnn()\n",
    "\n",
    "adam = optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "fcn.compile(loss=softmax_sparse_crossentropy_ignoring_last_label,\n",
    "              optimizer=adam,\n",
    "              metrics=[sparse_accuracy_ignoring_last_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look up simple_cnn weights, do not need to run\n",
    "\n",
    "layers = simple_cnn.layers\n",
    "for i in range(len(layers)):\n",
    "    n = len(layers[i].get_weights())\n",
    "    print(str(i) + \"-layer weight len: \", n)\n",
    "    if n == 2:\n",
    "        print(\"  weight matrix size: \", layers[i].get_weights()[0].shape)\n",
    "        print(\"  bias vector size: \", layers[i].get_weights()[1].shape)\n",
    "        print(layers[i].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(fcn, to_file='fcn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look up fcn weights, do not need to run\n",
    "\n",
    "layers = fcn.layers\n",
    "for i in range(len(layers)):\n",
    "    n = len(layers[i].get_weights())\n",
    "    print(str(i) + \"-layer weight len: \", n)\n",
    "    if n == 2:\n",
    "        print(\"  weight matrix size: \", layers[i].get_weights()[0].shape)\n",
    "        print(\"  bias vector size: \", layers[i].get_weights()[1].shape)\n",
    "        print(layers[i].get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights in fcn from those in simple_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_cnn.load_weights('simple_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_weights(fcn, simple_cnn):\n",
    "    for i in range(5):\n",
    "        fcn.layers[i].set_weights(simple_cnn.layers[i].get_weights())\n",
    "        \n",
    "set_weights(fcn, simple_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look up simple_cnn initialized weights, do not need to run\n",
    "\n",
    "layers = fcn.layers\n",
    "for i in range(len(layers)):\n",
    "    n = len(layers[i].get_weights())\n",
    "    print(str(i) + \"-layer weight len: \", n)\n",
    "    if n == 2:\n",
    "        print(\"  weight matrix size: \", layers[i].get_weights()[0].shape)\n",
    "        print(\"  bias vector size: \", layers[i].get_weights()[1].shape)\n",
    "        print(layers[i].get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read small image and mask files for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read training data\n",
    "\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "data_gen_args = dict(featurewise_center=True,\n",
    "                     featurewise_std_normalization=True,\n",
    "                     rotation_range=180.,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 1\n",
    "imgs_filename = [\"data_fcn_full/train/images/images/\"+str(i)+\".jpg\" for i in range(1, 14401)]\n",
    "masks_filename = [\"data_fcn_full/train/masks/masks/\"+str(i)+\"_mask.jpg\" for i in range(1, 14401)]\n",
    "sample_imgs = [np.expand_dims(io.imread(img_name), -1) for img_name in imgs_filename]\n",
    "sample_masks = [np.expand_dims(io.imread(mask_name), -1) for mask_name in masks_filename]\n",
    "image_datagen.fit(sample_imgs, augment=True, seed=seed)\n",
    "mask_datagen.fit(sample_masks, augment=True, seed=seed)\n",
    "\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    'data_fcn_full/train/images',\n",
    "    target_size=(70, 116),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "mask_generator = mask_datagen.flow_from_directory(\n",
    "    'data_fcn_full/train/masks',\n",
    "    target_size=(70, 116),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read validation data\n",
    "\n",
    "data_gen_args = dict(featurewise_center=True,\n",
    "                     featurewise_std_normalization=True,\n",
    "                     rotation_range=180.,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 1\n",
    "imgs_filename = [\"data_fcn_full/validation/images/images/\"+str(i)+\".jpg\" for i in range(1, 3571)]\n",
    "masks_filename = [\"data_fcn_full/validation/masks/masks/\"+str(i)+\"_mask.jpg\" for i in range(1, 3571)]\n",
    "sample_imgs = [np.expand_dims(io.imread(img_name), -1) for img_name in imgs_filename]\n",
    "sample_masks = [np.expand_dims(io.imread(mask_name), -1) for mask_name in masks_filename]\n",
    "image_datagen.fit(sample_imgs, augment=True, seed=seed)\n",
    "mask_datagen.fit(sample_masks, augment=True, seed=seed)\n",
    "\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    'data_fcn_full/validation/images',\n",
    "    target_size=(70, 116),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "mask_generator = mask_datagen.flow_from_directory(\n",
    "    'data_fcn_full/validation/masks',\n",
    "    target_size=(70, 116),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "validation_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN model training on small images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time-consuming training process of fcn, expect to run for a long time\n",
    "\n",
    "fcn.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=1500000//256,\n",
    "    epochs=2,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=150000//256)\n",
    "fcn.save_weights('fcn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN model prediction on small images demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = io.imread('data_fcn_full/validation/images/images/68.jpg') # numpy.ndarray [70, 116]\n",
    "mask = io.imread('data_fcn_full/validation/masks/masks/68_mask.jpg')\n",
    "img = np.expand_dims(img, 0)\n",
    "img = np.expand_dims(img, -1)\n",
    "pred = fcn.predict(img) # numpy.ndarray [1, 70, 116, 2]\n",
    "\n",
    "# print(pred[0, :10, :10, :])\n",
    "\n",
    "img = np.squeeze(img)\n",
    "pred = np.argmax(pred, 3) # numpy.ndarray [1, 70, 116]\n",
    "pred = np.squeeze(pred, 0) # numpy.ndaaray [70, 116]\n",
    "\n",
    "# print(pred[:10, :10])\n",
    "\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(pred)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask detection in original images"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
